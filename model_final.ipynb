{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import cpi\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m cpi_d \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1913\u001b[39m, \u001b[38;5;241m2024\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     cpi_d[year] \u001b[38;5;241m=\u001b[39m \u001b[43mcpi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minflate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m cpi_d[\u001b[38;5;241m1911\u001b[39m] \u001b[38;5;241m=\u001b[39m cpi_d[\u001b[38;5;241m1913\u001b[39m] \u001b[38;5;66;03m# The api only has data to 1913, so 1 movie released in 1911 is not included\u001b[39;00m\n\u001b[0;32m      9\u001b[0m cpi_d[\u001b[38;5;241m2024\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1.024\u001b[39m \u001b[38;5;66;03m# Bring 2024 down to 2023\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cpi\\__init__.py:163\u001b[0m, in \u001b[0;36minflate\u001b[1;34m(value, year_or_month, to, survey, seasonally_adjusted, periodicity, area, items, series_id)\u001b[0m\n\u001b[0;32m    154\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurvey\u001b[39m\u001b[38;5;124m\"\u001b[39m: survey,\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseasonally_adjusted\u001b[39m\u001b[38;5;124m\"\u001b[39m: seasonally_adjusted,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: series_id,\n\u001b[0;32m    161\u001b[0m }\n\u001b[0;32m    162\u001b[0m source_index \u001b[38;5;241m=\u001b[39m get(year_or_month, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m target_index \u001b[38;5;241m=\u001b[39m \u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (value \u001b[38;5;241m*\u001b[39m target_index) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(source_index)\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cpi\\__init__.py:76\u001b[0m, in \u001b[0;36mget\u001b[1;34m(year_or_month, survey, seasonally_adjusted, periodicity, area, items, series_id)\u001b[0m\n\u001b[0;32m     73\u001b[0m     series_obj \u001b[38;5;241m=\u001b[39m series\u001b[38;5;241m.\u001b[39mget_by_id(series_id)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Otherwise, we build the series id using the more humanized options\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     series_obj \u001b[38;5;241m=\u001b[39m \u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurvey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonally_adjusted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiodicity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Prep the lookup value depending on the input type.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(year_or_month, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cpi\\models.py:437\u001b[0m, in \u001b[0;36mSeriesList.get\u001b[1;34m(self, survey, seasonally_adjusted, periodicity, area, items)\u001b[0m\n\u001b[0;32m    428\u001b[0m series_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    429\u001b[0m     survey_code,\n\u001b[0;32m    430\u001b[0m     seasonality_code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    433\u001b[0m     Item\u001b[38;5;241m.\u001b[39mget_by_name(items)\u001b[38;5;241m.\u001b[39mcode,\n\u001b[0;32m    434\u001b[0m )\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Pull the series\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_by_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cpi\\models.py:321\u001b[0m, in \u001b[0;36mSeries.get_by_id\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m    319\u001b[0m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiodicity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Periodicity\u001b[38;5;241m.\u001b[39mget_by_id(d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiodicity\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    320\u001b[0m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Area\u001b[38;5;241m.\u001b[39mget_by_id(d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 321\u001b[0m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mItem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_by_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# Get the indexes\u001b[39;00m\n\u001b[0;32m    324\u001b[0m dict_list \u001b[38;5;241m=\u001b[39m query(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindexes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m WHERE series=?\u001b[39m\u001b[38;5;124m\"\u001b[39m, (value,))\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cpi\\models.py:101\u001b[0m, in \u001b[0;36mBaseObject.get_by_id\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_by_id\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the object with the provided identifier code.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mqueryone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * from \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m WHERE id=?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md)\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cpi\\models.py:73\u001b[0m, in \u001b[0;36mqueryone\u001b[1;34m(sql, params)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqueryone\u001b[39m(sql: \u001b[38;5;28mstr\u001b[39m, params: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Query the cpi.db database and return a single result.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m        {'id': '0000', 'code': 'US', 'name': 'United States'}\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     dict_list \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dict_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\cpi\\models.py:37\u001b[0m, in \u001b[0;36mquery\u001b[1;34m(sql, params)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Connect\u001b[39;00m\n\u001b[0;32m     36\u001b[0m this_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mabsolute()\n\u001b[1;32m---> 37\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpi.db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Query the sql\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\functools.py:477\u001b[0m, in \u001b[0;36mlru_cache\u001b[1;34m(maxsize, typed)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _HashedSeq(key)\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, typed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Least-recently-used cache decorator.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m    If *maxsize* is set to None, the LRU features are disabled and the cache\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;66;03m# Users should only access the lru_cache through its public API:\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m#       cache_info, cache_clear, and f.__wrapped__\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# The internals of the lru_cache are encapsulated for thread safety and\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# to allow the implementation to change (including a possible C version).\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Inflation Dictionary\n",
    "\n",
    "cpi_d = {}\n",
    "\n",
    "for year in range(1913, 2024):\n",
    "    cpi_d[year] = cpi.inflate(1, year, to = 2023)\n",
    "    \n",
    "cpi_d[1911] = cpi_d[1913] # The api only has data to 1913, so 1 movie released in 1911 is not included\n",
    "cpi_d[2024] = 1/1.024 # Bring 2024 down to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Max\\AppData\\Local\\Temp\\ipykernel_16092\\3457028901.py:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  df = pd.read_parquet('..\\Data\\IMDB.parquet')\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "df = pd.read_parquet('..\\Data\\IMDB.parquet') \n",
    "\n",
    "df = df.loc[(df.status == 'Released') & # Only include Released Movies\n",
    "            (df.adult == False) & # Filter out adult films\n",
    "            (df.vote_count > 0) & # Only include films with viewer reviews\n",
    "            (df.revenue > 0) & # Only include revenue generating movies\n",
    "            (pd.notna(df.release_date))] # Only include movies with release dates (included to resolve a small number of error cases)\n",
    "\n",
    "df['release_date'] = pd.to_datetime(df.release_date, format = '%Y-%m-%d') # Convert Release Date to a Date Time Object\n",
    "\n",
    "df['release_year'] = df.release_date.dt.year # Pull year out of release date\n",
    "df['release_month'] = df.release_date.dt.month # Pull month out of release date \n",
    "\n",
    "df['inflation_factor'] = df['release_year'].map(cpi_d) # Determine an inflation factor for each year\n",
    "\n",
    "df['adjusted_revenue'] = df['revenue'] * df['inflation_factor'] # Adjust Revenue for inflation\n",
    "df['adjusted_budget'] = df['budget'] * df['inflation_factor'] # Adjust Budget for inflation\n",
    "\n",
    "df['original_english'] = df['original_language'] == 'en' # Feature for if a movie's original language is english\n",
    "\n",
    "df.loc[pd.isna(df.Certificate), 'Certificate'] = 'None' # Mark Movies that do not have a Rating (ex. R, PG)\n",
    "\n",
    "ratings_counts = df['Certificate'].value_counts()\n",
    "rating_filter = lambda x: x if ratings_counts[x] > 100 else 'Other' # Only include commonly used Ratings\n",
    "\n",
    "df['Certificate'] = df['Certificate'].map(rating_filter) # Apply Rating Filter\n",
    "\n",
    "stars = lambda x: 'Many' if pd.notna(x.Star2) else ('One' if pd.notna(x.Star1) else 'None') # Mark Movies having 0, 1, or many star actors\n",
    "df['stars'] = df.apply(stars, axis = 1) # Apply Star Function\n",
    "\n",
    "df['listed_writer'] = pd.notna(df.Writer) # Feature tracking if the writer is listed\n",
    "df['listed_photography'] = pd.notna(df.Director_of_Photography) # Feature tracking if the D.O.P is listed\n",
    "df['listed_producers'] = pd.notna(df.Producers) # Feature tracking if the producer is listed\n",
    "df['listed_composer'] = pd.notna(df.Music_Composer) # Feature tracking if the composer is listed\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['keyword_sentiment'] = [sia.polarity_scores(x)['compound'] for x in df.keywords] # Get Sentiment Analysis for Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds\n",
    "    # The minimum amount of values each token should have in the dataset to be included\n",
    "\n",
    "lang_threshold = 500\n",
    "com_threshold = 100\n",
    "country_threshold = 100\n",
    "keyword_threshold = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Languages\n",
    "\n",
    "df.loc[df.spoken_languages.isna(), 'spoken_languages'] = 'Missing' # Mark observations with no listed language\n",
    "df.loc[df.spoken_languages == 'No Language', 'spoken_languages'] = 'None' # Mark silent films\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_lang = CountVectorizer( analyzer='word' ) # Vectorize by work\n",
    "\n",
    "vector_lang = vectorizer_lang.fit_transform(df.spoken_languages)\n",
    "\n",
    "dfLanguages = pd.DataFrame(vector_lang.toarray(), columns=vectorizer_lang.get_feature_names_out() )\n",
    "\n",
    "dfLanguages = dfLanguages.loc[:,dfLanguages.sum() > lang_threshold] # Only include languages that appear more frequently than the threshold\n",
    "dfLanguages['Other'] = dfLanguages.sum(axis = 1) < 1 # Mark other languages as other\n",
    "\n",
    "dfLanguages = dfLanguages.add_prefix('language_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Production Companies\n",
    "\n",
    "df.loc[df.production_companies.isna(), 'production_companies'] = 'Missing'\n",
    "\n",
    "comma_split = lambda x: [company.strip() for company in x.split(',')]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_com = CountVectorizer(tokenizer=comma_split)\n",
    "\n",
    "vector_com = vectorizer_com.fit_transform(df.production_companies)\n",
    "\n",
    "dfProdCom = pd.DataFrame(vector_com.toarray(), columns=vectorizer_com.get_feature_names_out() )\n",
    "\n",
    "dfProdCom = dfProdCom.loc[:,dfProdCom.sum() > com_threshold]\n",
    "\n",
    "dfProdCom['Other'] = dfProdCom.sum(axis = 1) < 1\n",
    "\n",
    "dfProdCom = dfProdCom.add_prefix('prod_company_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Production Countries\n",
    "\n",
    "df.loc[df.production_countries.isna(), 'production_countries'] = 'Missing'\n",
    "\n",
    "comma_split = lambda x: [company.strip() for company in x.split(',')]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_country = CountVectorizer(tokenizer=comma_split)\n",
    "\n",
    "vector_country = vectorizer_country.fit_transform(df.production_countries)\n",
    "\n",
    "dfCountry = pd.DataFrame(vector_country.toarray(), columns=vectorizer_country.get_feature_names_out() )\n",
    "\n",
    "dfCountry = dfCountry.loc[:,dfCountry.sum() > country_threshold]\n",
    "\n",
    "dfCountry['Other'] = dfCountry.sum(axis = 1) < 1\n",
    "\n",
    "dfCountry = dfCountry.add_prefix('prod_country_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_key = CountVectorizer(token_pattern = r'\\'([^\\']*)\\'')\n",
    "\n",
    "vector_key = vectorizer_key.fit_transform(df.keywords)\n",
    "\n",
    "dfkeyword = pd.DataFrame(vector_key.toarray(), columns=vectorizer_key.get_feature_names_out() )\n",
    "\n",
    "dfkeyword = dfkeyword.loc[:,dfkeyword.sum() > keyword_threshold]\n",
    "dfkeyword['No_Keywords'] = dfkeyword.sum(axis = 1) < 1\n",
    "\n",
    "dfkeyword = dfkeyword.add_prefix('keyword_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Columns\n",
    "\n",
    "final_columns = ['vote_average', 'vote_count', 'release_year','release_month', 'adjusted_revenue', 'runtime', \n",
    "                 'adjusted_budget', 'original_english', 'popularity', 'Certificate', 'listed_writer', \n",
    "                 'listed_photography', 'listed_producers', 'listed_composer','overview_sentiment','keyword_sentiment']\n",
    "\n",
    "final_df = pd.concat([df[final_columns].reset_index(drop=True), # Dataframe of final used columns\n",
    "                      dfProdCom.reset_index(drop=True), \n",
    "                      dfCountry.reset_index(drop=True), \n",
    "                      dfkeyword.reset_index(drop=True),\n",
    "                      dfLanguages.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Pipeline Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "np.random.seed(4767)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df, final_df.adjusted_revenue, test_size = 0.30)\n",
    "\n",
    "listCat = ['release_month','Certificate']\n",
    "listSkew = ['vote_average','vote_count', 'adjusted_budget'] # List of skewed numeric variables\n",
    "listNum = [col for col in final_df.columns if col not in (listCat + listSkew + ['adjusted_revenue'])]\n",
    "\n",
    "pipeCat = Pipeline([\n",
    "    ('selector', ColumnTransformer([('selector', 'passthrough', listCat)])),\n",
    "    ('encoder', OneHotEncoder(dtype=int, drop=\"first\", sparse_output= False))\n",
    "])\n",
    "\n",
    "pipeNum = Pipeline([\n",
    "    ('selector', ColumnTransformer([('selector','passthrough', listNum)])),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeSkew = Pipeline( [\n",
    "    ('selector', ColumnTransformer([ ('selector', 'passthrough', listSkew ) ] )),\n",
    "    ('spline',   PowerTransformer() ),\n",
    "    ('scaler',   StandardScaler() )\n",
    "])\n",
    "\n",
    "preprocessor = FeatureUnion([\n",
    "    ('cat', pipeCat),\n",
    "    ('num', pipeNum),\n",
    "    ('skew', pipeSkew)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [ 3.33177298e-001  4.68404998e-001  5.26844368e-001  5.50134819e-001\n",
      "  9.06505205e-002  2.33152960e-001  3.31305241e-001  3.94184155e-001\n",
      "  1.61406163e-002  1.61406163e-002  1.61406163e-002  1.61406163e-002\n",
      "  3.51394606e-001  4.86313692e-001  5.42685297e-001  5.68476302e-001\n",
      "  5.87526921e-001  5.90964697e-001  5.89917132e-001  5.84499058e-001\n",
      "  5.20974626e-001  5.35863983e-001  5.46510855e-001  5.58207225e-001\n",
      "  4.23632474e-002  4.23632474e-002  4.23632474e-002  4.23632474e-002\n",
      "  5.56094427e-001  5.51496738e-001  5.50647447e-001  5.49807574e-001\n",
      "  2.67758981e-001  2.53446687e-001  2.48505193e-001  2.48202841e-001\n",
      "  5.38867903e-001  5.44435351e-001  5.42973267e-001  5.42505513e-001\n",
      "  9.11305075e-002  9.11305075e-002  9.11305075e-002  9.11305075e-002\n",
      "              nan              nan              nan              nan\n",
      " -8.40752050e+058 -8.40752050e+058 -8.40752050e+058 -8.40752050e+058\n",
      " -2.68224700e+094 -7.12442324e+189 -1.89234647e+285             -inf\n",
      " -1.05581037e+003 -1.05581037e+003 -1.05581037e+003 -1.05581037e+003\n",
      " -4.72495852e+161 -4.72495852e+161 -4.72495852e+161 -4.72495852e+161\n",
      " -4.28768473e+059 -4.28768473e+059 -4.28768473e+059 -4.28768473e+059\n",
      " -4.86977993e+198             -inf             -inf              nan\n",
      " -5.03050538e+041 -5.03050538e+041 -5.03050538e+041 -5.03050538e+041\n",
      "              nan              nan              nan              nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1113: RuntimeWarning: invalid value encountered in subtract\n",
      "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
      "c:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1113: RuntimeWarning: overflow encountered in square\n",
      "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   1.4s\n",
      "Best Parameters: {'model__learning_rate': 0.1, 'model__loss': 'squared_error', 'model__max_iter': 100}\n",
      "Training: 0.7802154690769751\n",
      "Test: 0.6814642136890755\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Gradient Boosting Model\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "pipeHGBR = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('model', HistGradientBoostingRegressor()) \n",
    "], verbose=True)\n",
    "\n",
    "paramGridHGBR = {'model__loss': ['squared_error','absolute_error', 'gamma','poisson'], \n",
    "                 'model__learning_rate': [0.01,.1,1,10,100],\n",
    "                 'model__max_iter': [50,100,150,200],\n",
    "                 }\n",
    "\n",
    "gridHGBR = GridSearchCV(pipeHGBR,\n",
    "                        paramGridHGBR,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1,\n",
    "                        verbose = 4,\n",
    "                        scoring = 'r2')\n",
    "\n",
    "gridHGBR.fit(X_train, y_train)\n",
    "\n",
    "predTrainHGBR = gridHGBR.predict(X_train)\n",
    "predTestHGBR = gridHGBR.predict(X_test)\n",
    "\n",
    "train_HGBR_r2 = sklearn.metrics.r2_score(y_train, predTrainHGBR)\n",
    "test_HGBR_r2 = sklearn.metrics.r2_score(y_test, predTestHGBR)\n",
    "\n",
    "print(\"Best Parameters:\", gridHGBR.best_params_)\n",
    "print(f\"Training: {train_HGBR_r2}\\nTest: {test_HGBR_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=  20.9s\n",
      "Best Parameters: {'model__C': 10, 'model__epsilon': 1, 'model__kernel': 'linear'}\n",
      "Training: -0.08885226146197178\n",
      "Test: -0.10232736156776512\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Support Vector Regression\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipeSVR = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('model', SVR()) \n",
    "], verbose=True)\n",
    "\n",
    "paramGridSVR = {'model__kernel': ['linear','poly','rbf'], \n",
    "                'model__C': [.1,1,10],\n",
    "                'model__epsilon': [.01,.1,1],\n",
    "                 }\n",
    "\n",
    "gridSVR = GridSearchCV(pipeSVR,\n",
    "                        paramGridSVR,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1,\n",
    "                        verbose = 4,\n",
    "                        scoring = 'r2')\n",
    "\n",
    "gridSVR.fit(X_train, y_train)\n",
    "\n",
    "predTrainSVR = gridSVR.predict(X_train)\n",
    "predTestSVR = gridSVR.predict(X_test)\n",
    "\n",
    "train_SVR_r2 = sklearn.metrics.r2_score(y_train, predTrainSVR)\n",
    "test_SVR_r2 = sklearn.metrics.r2_score(y_test, predTestSVR)\n",
    "\n",
    "print(\"Best Parameters:\", gridSVR.best_params_)\n",
    "print(f\"Training: {train_SVR_r2}\\nTest: {test_SVR_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Best Parameters: {'model__algorithm': 'auto', 'model__n_neighbors': 9, 'model__weights': 'distance'}\n",
      "Training: 0.9999999999999655\n",
      "Test: 0.17628344302502008\n"
     ]
    }
   ],
   "source": [
    "# Model 3: KNeighbors Regression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "pipeKN = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('model', KNeighborsRegressor()) \n",
    "], verbose=True)\n",
    "\n",
    "paramGridKN = {'model__n_neighbors': [3,6,9], \n",
    "               'model__weights': ['uniform','distance'],\n",
    "               'model__algorithm': ['auto','ball_tree','kd_tree','brute'],\n",
    "              }\n",
    "\n",
    "gridKN = GridSearchCV(pipeKN,\n",
    "                        paramGridKN,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1,\n",
    "                        verbose = 4,\n",
    "                        scoring = 'r2')\n",
    "\n",
    "gridKN.fit(X_train, y_train)\n",
    "\n",
    "predTrainKN = gridKN.predict(X_train)\n",
    "predTestKN = gridKN.predict(X_test)\n",
    "\n",
    "train_KN_r2 = sklearn.metrics.r2_score(y_train, predTrainKN)\n",
    "test_KN_r2 = sklearn.metrics.r2_score(y_test, predTestKN)\n",
    "\n",
    "print(\"Best Parameters:\", gridKN.best_params_)\n",
    "print(f\"Training: {train_KN_r2}\\nTest: {test_KN_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 22\u001b[0m\n\u001b[0;32m     10\u001b[0m paramGridMLP \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel__hidden_layer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;241m2\u001b[39m,)],\n\u001b[0;32m     11\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel__learning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvscaling\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madaptive\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel__max_iter\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m5000\u001b[39m]\n\u001b[0;32m     13\u001b[0m               }\n\u001b[0;32m     15\u001b[0m gridMLP \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeMLP,\n\u001b[0;32m     16\u001b[0m                         paramGridMLP,\n\u001b[0;32m     17\u001b[0m                         cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     18\u001b[0m                         n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     19\u001b[0m                         verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     20\u001b[0m                         scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mgridMLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m predTrainMLP \u001b[38;5;241m=\u001b[39m gridKN\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m     25\u001b[0m predTestMLP \u001b[38;5;241m=\u001b[39m gridKN\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model 4: Neural Network\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "pipeMLP = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('model', MLPRegressor()) \n",
    "], verbose=True)\n",
    "\n",
    "paramGridMLP = {'model__hidden_layer_sizes': [(2,)],\n",
    "               'model__learning_rate': ['constant','invscaling','adaptive'],\n",
    "               'model__max_iter':[200] # Brought as high as 1000, still did not converge\n",
    "              }\n",
    "\n",
    "gridMLP = GridSearchCV(pipeMLP,\n",
    "                        paramGridMLP,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1,\n",
    "                        verbose = 4,\n",
    "                        scoring = 'r2')\n",
    "\n",
    "gridMLP.fit(X_train, y_train)\n",
    "\n",
    "predTrainMLP = gridKN.predict(X_train)\n",
    "predTestMLP = gridKN.predict(X_test)\n",
    "\n",
    "train_MLP_r2 = sklearn.metrics.r2_score(y_train, predTrainMLP)\n",
    "test_MLP_r2 = sklearn.metrics.r2_score(y_test, predTestMLP)\n",
    "\n",
    "print(\"Best Parameters:\", gridMLP.best_params_)\n",
    "print(f\"Training: {train_MLP_r2}\\nTest: {test_MLP_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.2s\n",
      "Training: 0.3406094010450834\n",
      "Test: 0.36376913129788147\n"
     ]
    }
   ],
   "source": [
    "# Model 5: Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipeLR = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "], verbose=True)\n",
    "\n",
    "pipeLR.fit(X_train, y_train)\n",
    "\n",
    "predTrainLR = pipeLR.predict(X_train)\n",
    "predTestLR = pipeLR.predict(X_test)\n",
    "\n",
    "train_LR_r2 = sklearn.metrics.r2_score(y_train, predTrainLR)\n",
    "test_LR_r2 = sklearn.metrics.r2_score(y_test, predTestLR)\n",
    "\n",
    "print(f\"Training: {train_LR_r2}\\nTest: {test_LR_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
